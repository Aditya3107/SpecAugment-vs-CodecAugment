{"cells":[{"metadata":{"_uuid":"2ef92f84-f062-4a26-8a61-cb5bc345e0cd","_cell_guid":"ee34c6a9-771a-40ad-9995-38c171b53b60","trusted":true,"_kg_hide-input":false,"scrolled":false},"cell_type":"code","source":"from __future__ import print_function\nimport pandas as pd\nimport numpy as np\nimport os\nimport sys\n\n#sklearn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n#keras\nimport keras\nimport tensorflow as tf\nfrom keras import regularizers\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential, Model, model_from_json\nfrom keras.layers import Dense, Embedding, LSTM\nfrom keras.layers import Input, Flatten, Dropout, Activation, BatchNormalization\nfrom keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\nfrom keras.utils import np_utils, to_categorical\nfrom keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.callbacks import TensorBoard\nfrom tensorflow.keras import backend as K\n\n#skopt\nimport skopt\nfrom skopt import gp_minimize, forest_minimize\nfrom skopt.space import Real, Categorical, Integer\nfrom skopt.plots import plot_convergence\nfrom skopt.plots import plot_objective, plot_evaluations\nfrom skopt.plots import plot_histogram, plot_objective_2D\nfrom skopt.utils import use_named_args\n\n\n\ndef feature_dataframe_gen_train(feature_dir):\n    feature_dataframe = []\n    for sess in range(1,5) :\n        feature_df = pd.read_csv('{}audio_features_{}.csv'.format(feature_dir,sess))\n        feature_df = feature_df.sample(frac=1,random_state = 50).reset_index(drop = True)\n        feature_dataframe.append(feature_df)\n    feature_dataframe = (pd.concat(feature_dataframe)).fillna(0)\n    feature_dataframe = feature_dataframe.replace('exc','hap')\n    #feature_dataframe = feature_dataframe.drop(feature_dataframe[feature_dataframe.emotions.isin([\"sur\", \"fea\"])].index)\n    return feature_dataframe\n\ndef feature_dataframe_codec_train(feature_dir):\n    feature_dataframe = []\n    for sess in range(1,5) :\n        feature_df = pd.read_csv('{}audio_features_{}_{}.csv'.format(feature_dir,bitrate,sess))\n        feature_df = feature_df.sample(frac=1,random_state = 50).reset_index(drop = True)\n        feature_dataframe.append(feature_df)\n    feature_dataframe = (pd.concat(feature_dataframe)).fillna(0)\n    feature_dataframe = feature_dataframe.replace('exc','hap')\n    #feature_dataframe = feature_dataframe.drop(feature_dataframe[feature_dataframe.emotions.isin([\"sur\", \"fea\"])].index)\n    return feature_dataframe\n\ndef feature_dataframe_gen_test(feature_dir):\n    feature_dataframe = []\n    for sess in [5] :\n        feature_df = pd.read_csv('{}audio_features_{}.csv'.format(feature_dir,sess))\n        feature_df = feature_df.sample(frac=1,random_state = 50).reset_index(drop = True)\n        feature_dataframe.append(feature_df)\n    feature_dataframe = (pd.concat(feature_dataframe)).fillna(0)\n    feature_dataframe = feature_dataframe.replace('exc','hap')\n    #feature_dataframe = feature_dataframe.drop(feature_dataframe[feature_dataframe.emotions.isin([\"sur\", \"fea\"])].index)\n    return feature_dataframe\n\n# This is a function to log traning progress so that can be viewed by TnesorBoard.\ndef log_dir_name(learning_rate, num_dense_layers,\n                 filters, dropout, momentum):\n    # The dir-name for the TensorBoard log-dir.\n    s = \"./19_logs/lr_{0:.0e}_layers_{1}_nodes_{2}_{3}/\"\n    # Insert all the hyper-parameters in the dir-name.\n    log_dir = s.format(learning_rate, num_dense_layers,\n                       filters, dropout, momentum)\n\n    return log_dir\n\n\ndim_learning_rate = Real(low=1e-6, high=1e-2, prior='log-uniform',name='learning_rate')\ndim_filters = Integer(low = 32, high = 512, name = 'filters')\n#dim_kernel_size = Integer(low = 4, high = 8, name= 'kernel_size')\ndim_dropout = Real(low = 0.1, high = 0.6, name = 'dropout')\ndim_momentum = Real(low = 0.0, high = 0.99, name = 'momentum')\n#dim_activation = Categorical(categories=['relu', 'sigmoid'],name='activation')\n    \n    \ndim_num_dense_layers = Integer(low=1, high=4, name='num_dense_layers')\n\n    \n    \ndimensions = [dim_learning_rate,\n              dim_num_dense_layers,\n              #dim_activation,\n              dim_filters,\n              dim_dropout,\n              dim_momentum]\n    \ndefault_parameters = [0.0001,2,256,0.5,0.0]\n\ndef create_model(learning_rate, num_dense_layers,\n                 filters, dropout, momentum):\n    \n    model = Sequential()\n    model.add(Conv1D(256, 8, padding='same',input_shape=(X_train.shape[1],1)))\n    model.add(Activation(activation = 'relu'))\n    model.add(Conv1D(filters = filters, kernel_size = 8,padding = 'same'))\n    model.add(BatchNormalization()) \n    model.add(Activation(activation = 'relu'))\n    model.add(Dropout(rate=dropout))\n    model.add(MaxPooling1D(pool_size=(8)))\n    for i in range(num_dense_layers):\n        name = 'layer2_Conv1D_{0}'.format(i+1)\n        model.add(Conv1D(filters = filters,kernel_size = 8 ,padding = 'same', name = name))\n        model.add(Activation(activation = 'relu'))\n    model.add(BatchNormalization())\n    model.add(Activation(activation = 'relu'))\n    model.add(Dropout(rate=dropout))\n    model.add(MaxPooling1D(pool_size=(8)))\n    for i in range(num_dense_layers):\n        name = 'layer3_Conv1D_{0}'.format(i+1)\n        model.add(Conv1D(filters = filters, kernel_size = 8,padding = 'same', name = name))\n        model.add(Activation(activation = 'relu'))\n    model.add(Flatten())\n    model.add(Dense(4, activation=\"softmax\"))\n    opt = keras.optimizers.SGD(learning_rate = learning_rate,momentum = momentum,nesterov=False,decay = 0.0)\n    model.compile(optimizer = opt,\n                 loss = keras.losses.CategoricalCrossentropy(),\n                 metrics = [tf.keras.metrics.Recall(),tf.keras.metrics.Precision(),'accuracy'])\n    \n    return model\n\n\nmodel_name = 'Emotion_recognition_iemocap.h5'\nbest_accuracy = 0.0\n\n@use_named_args(dimensions=dimensions)\ndef fitness(learning_rate, num_dense_layers,\n            filters, dropout, momentum):\n    \"\"\"\n    Hyper-parameters:\n    learning_rate:     Learning-rate for the optimizer.\n    num_dense_layers:  Number of dense layers.\n    activation:        Activation function for all layers.\n    \"\"\"\n\n    # Print the hyper-parameters.\n    print('learning rate: {0:.1e}'.format(learning_rate))\n    print('num_dense_layers:', num_dense_layers)\n    #print('activation:',activation)\n    print('filters:' ,filters)\n    print('dropout:' ,dropout)\n    print('momentum:' ,momentum)\n    print()\n    model = create_model(learning_rate = learning_rate,\n                         num_dense_layers = num_dense_layers,\n                         #activation = activation, \n                         filters = filters, \n                         dropout = dropout, \n                         momentum = momentum)\n    # Dir-name for the TensorBoard log-files.\n    log_dir = log_dir_name(learning_rate, num_dense_layers,\n                           filters, dropout, momentum)\n    \n    # Create a callback-function for Keras which will be\n    # run after each epoch has ended during training.\n    # This saves the log-files for TensorBoard.\n    # Note that there are complications when histogram_freq=1.\n    # It might give strange errors and it also does not properly\n    # support Keras data-generators for the validation-set.\n    callback_log = TensorBoard(\n        log_dir=log_dir,\n        histogram_freq=0,\n        write_graph=True,\n        write_grads=False,\n        write_images=False)\n   \n    # Use Keras to train the model.\n    history=model.fit(X_train, y_train, \n                      batch_size=16, \n                      epochs=60, \n                      validation_data=(X_test, y_test),\n                      callbacks=[callback_log])\n    # Get the classification accuracy on the validation-set\n    # after the last training-epoch.\n    accuracy = history.history['val_accuracy'][-1]\n\n    # Print the classification accuracy.\n    print()\n    print(\"Accuracy: {0:.2%}\".format(accuracy))\n    print()\n\n    # Save the model if it improves on the best-found performance.\n    # We use the global keyword so we update the variable outside\n    # of this function.\n    global best_accuracy\n    \n    save_dir = os.path.join(os.getcwd(),'saved_models')\n    if not os.path.isdir(save_dir):\n        os.mkdir(save_dir)\n    model_path = os.path.join(save_dir,model_name)\n    \n    # If the classification accuracy of the saved model is improved ...\n    if accuracy > best_accuracy:\n        # Save the new model to harddisk.\n        model.save(model_path)\n        model_json = model.to_json()\n        with open(\"model_json.json\", \"w\") as file:\n            file.write(model_json)\n        # Update the classification accuracy.\n        best_accuracy = accuracy\n\n    # Delete the Keras model with these hyper-parameters from memory.\n    del model\n    \n    # Clear the Keras session, otherwise it will keep adding new\n    # models to the same TensorFlow graph each time we create\n    # a model with a different set of hyper-parameters.\n    K.clear_session()\n    \n    # NOTE: Scikit-optimize does minimization so it tries to\n    # find a set of hyper-parameters with the LOWEST fitness-value.\n    # Because we are interested in the HIGHEST classification\n    # accuracy, we need to negate this number so it can be minimized.\n    return -accuracy\n\nif __name__ == '__main__':\n    #CodecAugment - 8,16,32,48,64,128 kbps\n    ori_feature_dataframe_train = feature_dataframe_gen_train(feature_dir = '../input/iemocap-audio-features/')\n    ori_feature_dataframe_train['fold'] = -1\n    #codec_feature_dataframe_train = feature_dataframe_codec_train(feature_dir = '../input/iemocap-compressed/')\n    #codec_feature_dataframe_train['fold'] = -1\n    ori_feature_dataframe_test = feature_dataframe_gen_test(feature_dir = '../input/iemocap-audio-features/')\n    ori_feature_dataframe_test['fold'] = 1\n    result = (pd.concat([ori_feature_dataframe_train,ori_feature_dataframe_test], ignore_index=True, sort=False)).fillna(0.0)\n    result = result.loc[:, (result==0.0).mean() < .9]\n    training_data = result[result['fold'] == -1]\n    testing_data = result[result['fold'] == 1]\n    X_train_data = training_data.drop(['emotions', 'fold'],axis=1)\n    X_test_data = testing_data.drop(['emotions', 'fold'],axis=1)\n    y_train_data = training_data.emotions\n    y_test_data = testing_data.emotions\n    scaler = StandardScaler()\n    std_scale = scaler.fit(X_train_data)\n    X_train = std_scale.transform(X_train_data)\n    X_test  = std_scale.transform(X_test_data)\n    y_train_ = np.array(y_train_data)\n    y_test_ = np.array(y_test_data)\n    lb = LabelEncoder()\n    y_train = np_utils.to_categorical(lb.fit_transform(y_train_))\n    y_test = np_utils.to_categorical(lb.fit_transform(y_test_))\n    X_train = np.expand_dims(X_train, axis = 2)\n    X_test = np.expand_dims(X_test, axis = 2)\n    search_result = gp_minimize(func=fitness,\n                            dimensions=dimensions,\n                            acq_func='EI', # Expected Improvement.\n                            n_calls=15,\n                            x0=default_parameters)   ","execution_count":null,"outputs":[{"output_type":"stream","text":"learning rate: 1.0e-04\nnum_dense_layers: 2\nfilters: 256\ndropout: 0.5\nmomentum: 0.0\n\nEpoch 1/60\n269/269 [==============================] - 13s 49ms/step - loss: 1.5076 - recall: 0.0737 - precision: 0.4208 - accuracy: 0.3387 - val_loss: 1.3793 - val_recall: 8.0580e-04 - val_precision: 1.0000 - val_accuracy: 0.3747\nEpoch 2/60\n269/269 [==============================] - 12s 46ms/step - loss: 1.3248 - recall: 0.0909 - precision: 0.5263 - accuracy: 0.4072 - val_loss: 1.3663 - val_recall: 0.0024 - val_precision: 0.3750 - val_accuracy: 0.4448\nEpoch 3/60\n269/269 [==============================] - 13s 48ms/step - loss: 1.2937 - recall: 0.0914 - precision: 0.5039 - accuracy: 0.4142 - val_loss: 1.3434 - val_recall: 0.0089 - val_precision: 0.5500 - val_accuracy: 0.4085\nEpoch 4/60\n269/269 [==============================] - 12s 46ms/step - loss: 1.2577 - recall: 0.1021 - precision: 0.5190 - accuracy: 0.4152 - val_loss: 1.3378 - val_recall: 0.0064 - val_precision: 0.4000 - val_accuracy: 0.4545\nEpoch 5/60\n269/269 [==============================] - 12s 46ms/step - loss: 1.2405 - recall: 0.1096 - precision: 0.5609 - accuracy: 0.4280 - val_loss: 1.3228 - val_recall: 0.0145 - val_precision: 0.6667 - val_accuracy: 0.4384\nEpoch 6/60\n269/269 [==============================] - 13s 47ms/step - loss: 1.2243 - recall: 0.1154 - precision: 0.5638 - accuracy: 0.4392 - val_loss: 1.3279 - val_recall: 0.0137 - val_precision: 0.8947 - val_accuracy: 0.4335\nEpoch 7/60\n269/269 [==============================] - 13s 47ms/step - loss: 1.2075 - recall: 0.1142 - precision: 0.5744 - accuracy: 0.4436 - val_loss: 1.3243 - val_recall: 0.0081 - val_precision: 0.5882 - val_accuracy: 0.4658\nEpoch 8/60\n269/269 [==============================] - 13s 47ms/step - loss: 1.2081 - recall: 0.1219 - precision: 0.5766 - accuracy: 0.4455 - val_loss: 1.3193 - val_recall: 0.0185 - val_precision: 0.7667 - val_accuracy: 0.4182\nEpoch 9/60\n269/269 [==============================] - 13s 47ms/step - loss: 1.2072 - recall: 0.1189 - precision: 0.5586 - accuracy: 0.4401 - val_loss: 1.3143 - val_recall: 0.0177 - val_precision: 0.8148 - val_accuracy: 0.4343\nEpoch 10/60\n269/269 [==============================] - 12s 46ms/step - loss: 1.1805 - recall: 0.1319 - precision: 0.5921 - accuracy: 0.4513 - val_loss: 1.3103 - val_recall: 0.0177 - val_precision: 0.7586 - val_accuracy: 0.4585\nEpoch 11/60\n269/269 [==============================] - 12s 46ms/step - loss: 1.1773 - recall: 0.1345 - precision: 0.5955 - accuracy: 0.4615 - val_loss: 1.3041 - val_recall: 0.0153 - val_precision: 0.7600 - val_accuracy: 0.4609\nEpoch 12/60\n269/269 [==============================] - 13s 48ms/step - loss: 1.1642 - recall: 0.1359 - precision: 0.6041 - accuracy: 0.4564 - val_loss: 1.3099 - val_recall: 0.0113 - val_precision: 0.7000 - val_accuracy: 0.4529\nEpoch 13/60\n269/269 [==============================] - 13s 47ms/step - loss: 1.1733 - recall: 0.1331 - precision: 0.5682 - accuracy: 0.4538 - val_loss: 1.3018 - val_recall: 0.0177 - val_precision: 0.7586 - val_accuracy: 0.4617\nEpoch 14/60\n269/269 [==============================] - 12s 46ms/step - loss: 1.1562 - recall: 0.1494 - precision: 0.6111 - accuracy: 0.4723 - val_loss: 1.3040 - val_recall: 0.0113 - val_precision: 0.5833 - val_accuracy: 0.4432\nEpoch 15/60\n269/269 [==============================] - 12s 46ms/step - loss: 1.1474 - recall: 0.1478 - precision: 0.6271 - accuracy: 0.4744 - val_loss: 1.3021 - val_recall: 0.0113 - val_precision: 0.6667 - val_accuracy: 0.4335\nEpoch 16/60\n269/269 [==============================] - 13s 47ms/step - loss: 1.1353 - recall: 0.1559 - precision: 0.6166 - accuracy: 0.4767 - val_loss: 1.3001 - val_recall: 0.0161 - val_precision: 0.7692 - val_accuracy: 0.4561\nEpoch 17/60\n269/269 [==============================] - 13s 47ms/step - loss: 1.1338 - recall: 0.1573 - precision: 0.6238 - accuracy: 0.4774 - val_loss: 1.3006 - val_recall: 0.0169 - val_precision: 0.8077 - val_accuracy: 0.4327\nEpoch 18/60\n269/269 [==============================] - 12s 46ms/step - loss: 1.1264 - recall: 0.1587 - precision: 0.6248 - accuracy: 0.4867 - val_loss: 1.2971 - val_recall: 0.0201 - val_precision: 0.8621 - val_accuracy: 0.4448\nEpoch 19/60\n269/269 [==============================] - 12s 46ms/step - loss: 1.1223 - recall: 0.1683 - precision: 0.6499 - accuracy: 0.4939 - val_loss: 1.2993 - val_recall: 0.0129 - val_precision: 0.6400 - val_accuracy: 0.4891\nEpoch 20/60\n269/269 [==============================] - 12s 46ms/step - loss: 1.1234 - recall: 0.1667 - precision: 0.6158 - accuracy: 0.4765 - val_loss: 1.2929 - val_recall: 0.0145 - val_precision: 0.7500 - val_accuracy: 0.4271\nEpoch 21/60\n269/269 [==============================] - 13s 47ms/step - loss: 1.1031 - recall: 0.1821 - precision: 0.6460 - accuracy: 0.4967 - val_loss: 1.2981 - val_recall: 0.0193 - val_precision: 0.8000 - val_accuracy: 0.4303\nEpoch 22/60\n269/269 [==============================] - 13s 47ms/step - loss: 1.1036 - recall: 0.1765 - precision: 0.6640 - accuracy: 0.4986 - val_loss: 1.2845 - val_recall: 0.0177 - val_precision: 0.7857 - val_accuracy: 0.4343\nEpoch 23/60\n269/269 [==============================] - 13s 46ms/step - loss: 1.0976 - recall: 0.1935 - precision: 0.6505 - accuracy: 0.5065 - val_loss: 1.2936 - val_recall: 0.0185 - val_precision: 0.8214 - val_accuracy: 0.4537\nEpoch 24/60\n269/269 [==============================] - 12s 46ms/step - loss: 1.0952 - recall: 0.1893 - precision: 0.6559 - accuracy: 0.5037 - val_loss: 1.2948 - val_recall: 0.0145 - val_precision: 0.8571 - val_accuracy: 0.4649\nEpoch 25/60\n269/269 [==============================] - 12s 46ms/step - loss: 1.0964 - recall: 0.1872 - precision: 0.6439 - accuracy: 0.4949 - val_loss: 1.2910 - val_recall: 0.0169 - val_precision: 0.8077 - val_accuracy: 0.4319\nEpoch 26/60\n269/269 [==============================] - 13s 48ms/step - loss: 1.0943 - recall: 0.1867 - precision: 0.6362 - accuracy: 0.5033 - val_loss: 1.2881 - val_recall: 0.0137 - val_precision: 0.7727 - val_accuracy: 0.4899\nEpoch 27/60\n269/269 [==============================] - 13s 47ms/step - loss: 1.0829 - recall: 0.1998 - precision: 0.6592 - accuracy: 0.5161 - val_loss: 1.2859 - val_recall: 0.0145 - val_precision: 0.7500 - val_accuracy: 0.4392\nEpoch 28/60\n269/269 [==============================] - 12s 46ms/step - loss: 1.0845 - recall: 0.2005 - precision: 0.6703 - accuracy: 0.5152 - val_loss: 1.2870 - val_recall: 0.0169 - val_precision: 0.7778 - val_accuracy: 0.4440\nEpoch 29/60\n269/269 [==============================] - 12s 46ms/step - loss: 1.0719 - recall: 0.2035 - precision: 0.6858 - accuracy: 0.5221 - val_loss: 1.2915 - val_recall: 0.0169 - val_precision: 0.8400 - val_accuracy: 0.4359\nEpoch 30/60\n269/269 [==============================] - 12s 46ms/step - loss: 1.0606 - recall: 0.2124 - precision: 0.6907 - accuracy: 0.5273 - val_loss: 1.2922 - val_recall: 0.0169 - val_precision: 0.8750 - val_accuracy: 0.4593\nEpoch 31/60\n269/269 [==============================] - 13s 47ms/step - loss: 1.0707 - recall: 0.2166 - precision: 0.6786 - accuracy: 0.5217 - val_loss: 1.2800 - val_recall: 0.0137 - val_precision: 0.7391 - val_accuracy: 0.4754\nEpoch 32/60\n269/269 [==============================] - 13s 47ms/step - loss: 1.0616 - recall: 0.2212 - precision: 0.6779 - accuracy: 0.5221 - val_loss: 1.2807 - val_recall: 0.0177 - val_precision: 0.7857 - val_accuracy: 0.4786\nEpoch 33/60\n269/269 [==============================] - 12s 46ms/step - loss: 1.0574 - recall: 0.2154 - precision: 0.6968 - accuracy: 0.5256 - val_loss: 1.2829 - val_recall: 0.0161 - val_precision: 0.7143 - val_accuracy: 0.4641\nEpoch 34/60\n269/269 [==============================] - 12s 46ms/step - loss: 1.0653 - recall: 0.2219 - precision: 0.6839 - accuracy: 0.5329 - val_loss: 1.2906 - val_recall: 0.0210 - val_precision: 0.8125 - val_accuracy: 0.4150\nEpoch 35/60\n269/269 [==============================] - 12s 46ms/step - loss: 1.0587 - recall: 0.2175 - precision: 0.6815 - accuracy: 0.5280 - val_loss: 1.2797 - val_recall: 0.0185 - val_precision: 0.7667 - val_accuracy: 0.4359\nEpoch 36/60\n269/269 [==============================] - 13s 48ms/step - loss: 1.0478 - recall: 0.2296 - precision: 0.6961 - accuracy: 0.5343 - val_loss: 1.2809 - val_recall: 0.0201 - val_precision: 0.7812 - val_accuracy: 0.4480\n","name":"stdout"},{"output_type":"stream","text":"Epoch 37/60\n269/269 [==============================] - 13s 47ms/step - loss: 1.0498 - recall: 0.2224 - precision: 0.6933 - accuracy: 0.5333 - val_loss: 1.2808 - val_recall: 0.0242 - val_precision: 0.7317 - val_accuracy: 0.4150\nEpoch 38/60\n269/269 [==============================] - 12s 46ms/step - loss: 1.0509 - recall: 0.2317 - precision: 0.6903 - accuracy: 0.5263 - val_loss: 1.2806 - val_recall: 0.0234 - val_precision: 0.7436 - val_accuracy: 0.4247\nEpoch 39/60\n269/269 [==============================] - 12s 46ms/step - loss: 1.0454 - recall: 0.2373 - precision: 0.6823 - accuracy: 0.5368 - val_loss: 1.2772 - val_recall: 0.0161 - val_precision: 0.6897 - val_accuracy: 0.4512\nEpoch 40/60\n269/269 [==============================] - 12s 46ms/step - loss: 1.0426 - recall: 0.2422 - precision: 0.6931 - accuracy: 0.5340 - val_loss: 1.2757 - val_recall: 0.0169 - val_precision: 0.7241 - val_accuracy: 0.4658\nEpoch 41/60\n269/269 [==============================] - 13s 47ms/step - loss: 1.0395 - recall: 0.2413 - precision: 0.6796 - accuracy: 0.5326 - val_loss: 1.2765 - val_recall: 0.0169 - val_precision: 0.7778 - val_accuracy: 0.4859\nEpoch 42/60\n269/269 [==============================] - 12s 46ms/step - loss: 1.0401 - recall: 0.2469 - precision: 0.6913 - accuracy: 0.5354 - val_loss: 1.2812 - val_recall: 0.0242 - val_precision: 0.7317 - val_accuracy: 0.4279\nEpoch 43/60\n269/269 [==============================] - 12s 46ms/step - loss: 1.0372 - recall: 0.2583 - precision: 0.6964 - accuracy: 0.5305 - val_loss: 1.2785 - val_recall: 0.0250 - val_precision: 0.7209 - val_accuracy: 0.4343\nEpoch 44/60\n269/269 [==============================] - 13s 47ms/step - loss: 1.0373 - recall: 0.2445 - precision: 0.6933 - accuracy: 0.5378 - val_loss: 1.2797 - val_recall: 0.0193 - val_precision: 0.7742 - val_accuracy: 0.4859\nEpoch 45/60\n269/269 [==============================] - 13s 46ms/step - loss: 1.0398 - recall: 0.2452 - precision: 0.6912 - accuracy: 0.5431 - val_loss: 1.2823 - val_recall: 0.0201 - val_precision: 0.7576 - val_accuracy: 0.4633\nEpoch 46/60\n269/269 [==============================] - 13s 47ms/step - loss: 1.0278 - recall: 0.2590 - precision: 0.6884 - accuracy: 0.5364 - val_loss: 1.2771 - val_recall: 0.0226 - val_precision: 0.7568 - val_accuracy: 0.4392\nEpoch 47/60\n269/269 [==============================] - 12s 46ms/step - loss: 1.0275 - recall: 0.2515 - precision: 0.6952 - accuracy: 0.5406 - val_loss: 1.2779 - val_recall: 0.0210 - val_precision: 0.7647 - val_accuracy: 0.4464\nEpoch 48/60\n269/269 [==============================] - 12s 46ms/step - loss: 1.0197 - recall: 0.2629 - precision: 0.7037 - accuracy: 0.5478 - val_loss: 1.2821 - val_recall: 0.0282 - val_precision: 0.7447 - val_accuracy: 0.4537\nEpoch 49/60\n269/269 [==============================] - 12s 46ms/step - loss: 1.0190 - recall: 0.2681 - precision: 0.7051 - accuracy: 0.5503 - val_loss: 1.2715 - val_recall: 0.0250 - val_precision: 0.7045 - val_accuracy: 0.4384\nEpoch 50/60\n269/269 [==============================] - 13s 47ms/step - loss: 1.0127 - recall: 0.2622 - precision: 0.7058 - accuracy: 0.5534 - val_loss: 1.2701 - val_recall: 0.0250 - val_precision: 0.7045 - val_accuracy: 0.4811\nEpoch 51/60\n269/269 [==============================] - 13s 47ms/step - loss: 1.0179 - recall: 0.2734 - precision: 0.6974 - accuracy: 0.5531 - val_loss: 1.2747 - val_recall: 0.0242 - val_precision: 0.7317 - val_accuracy: 0.4359\nEpoch 52/60\n269/269 [==============================] - 12s 46ms/step - loss: 1.0197 - recall: 0.2786 - precision: 0.6952 - accuracy: 0.5466 - val_loss: 1.2720 - val_recall: 0.0226 - val_precision: 0.7179 - val_accuracy: 0.4786\nEpoch 53/60\n269/269 [==============================] - 12s 46ms/step - loss: 1.0119 - recall: 0.2716 - precision: 0.6935 - accuracy: 0.5513 - val_loss: 1.2786 - val_recall: 0.0298 - val_precision: 0.7115 - val_accuracy: 0.4400\nEpoch 54/60\n269/269 [==============================] - 12s 46ms/step - loss: 1.0162 - recall: 0.2683 - precision: 0.6896 - accuracy: 0.5415 - val_loss: 1.2779 - val_recall: 0.0290 - val_precision: 0.7200 - val_accuracy: 0.4222\nEpoch 55/60\n269/269 [==============================] - 13s 47ms/step - loss: 1.0136 - recall: 0.2725 - precision: 0.6992 - accuracy: 0.5501 - val_loss: 1.2775 - val_recall: 0.0250 - val_precision: 0.7381 - val_accuracy: 0.4593\nEpoch 56/60\n269/269 [==============================] - 12s 46ms/step - loss: 1.0065 - recall: 0.2832 - precision: 0.7151 - accuracy: 0.5538 - val_loss: 1.2758 - val_recall: 0.0266 - val_precision: 0.7333 - val_accuracy: 0.4424\nEpoch 57/60\n269/269 [==============================] - 12s 46ms/step - loss: 1.0092 - recall: 0.2762 - precision: 0.7058 - accuracy: 0.5615 - val_loss: 1.2749 - val_recall: 0.0161 - val_precision: 0.7143 - val_accuracy: 0.4577\nEpoch 58/60\n269/269 [==============================] - 12s 46ms/step - loss: 1.0053 - recall: 0.2897 - precision: 0.7079 - accuracy: 0.5557 - val_loss: 1.2729 - val_recall: 0.0258 - val_precision: 0.7442 - val_accuracy: 0.4706\nEpoch 59/60\n269/269 [==============================] - 12s 46ms/step - loss: 1.0035 - recall: 0.2869 - precision: 0.7099 - accuracy: 0.5620 - val_loss: 1.2751 - val_recall: 0.0266 - val_precision: 0.7174 - val_accuracy: 0.4521\nEpoch 60/60\n269/269 [==============================] - 13s 48ms/step - loss: 0.9995 - recall: 0.2895 - precision: 0.7065 - accuracy: 0.5660 - val_loss: 1.2712 - val_recall: 0.0234 - val_precision: 0.6591 - val_accuracy: 0.4585\n\nAccuracy: 45.85%\n\nlearning rate: 2.0e-05\nnum_dense_layers: 4\nfilters: 337\ndropout: 0.303590275560641\nmomentum: 0.3203628761145875\n\nEpoch 1/60\n269/269 [==============================] - 23s 87ms/step - loss: 1.3869 - recall: 0.0287 - precision: 0.4331 - accuracy: 0.2730 - val_loss: 1.3834 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_accuracy: 0.3739\nEpoch 2/60\n269/269 [==============================] - 22s 81ms/step - loss: 1.3386 - recall: 0.0417 - precision: 0.5173 - accuracy: 0.3007 - val_loss: 1.3712 - val_recall: 0.0040 - val_precision: 0.8333 - val_accuracy: 0.4239\nEpoch 3/60\n269/269 [==============================] - 22s 82ms/step - loss: 1.3240 - recall: 0.0422 - precision: 0.4959 - accuracy: 0.3224 - val_loss: 1.3616 - val_recall: 0.0056 - val_precision: 0.7778 - val_accuracy: 0.4198\nEpoch 4/60\n269/269 [==============================] - 22s 81ms/step - loss: 1.3091 - recall: 0.0531 - precision: 0.5907 - accuracy: 0.3429 - val_loss: 1.3570 - val_recall: 0.0064 - val_precision: 0.8000 - val_accuracy: 0.4102\nEpoch 5/60\n269/269 [==============================] - 22s 82ms/step - loss: 1.3096 - recall: 0.0552 - precision: 0.6156 - accuracy: 0.3716 - val_loss: 1.3560 - val_recall: 0.0048 - val_precision: 0.8571 - val_accuracy: 0.4134\nEpoch 6/60\n269/269 [==============================] - 22s 82ms/step - loss: 1.2982 - recall: 0.0583 - precision: 0.6143 - accuracy: 0.3779 - val_loss: 1.3551 - val_recall: 0.0064 - val_precision: 0.8889 - val_accuracy: 0.4069\nEpoch 7/60\n269/269 [==============================] - 22s 82ms/step - loss: 1.2904 - recall: 0.0569 - precision: 0.6240 - accuracy: 0.3911 - val_loss: 1.3510 - val_recall: 0.0056 - val_precision: 0.6364 - val_accuracy: 0.4085\nEpoch 8/60\n269/269 [==============================] - 22s 82ms/step - loss: 1.2885 - recall: 0.0562 - precision: 0.6132 - accuracy: 0.4007 - val_loss: 1.3498 - val_recall: 0.0056 - val_precision: 0.7778 - val_accuracy: 0.4053\nEpoch 9/60\n269/269 [==============================] - 22s 82ms/step - loss: 1.2833 - recall: 0.0555 - precision: 0.6381 - accuracy: 0.4047 - val_loss: 1.3489 - val_recall: 0.0056 - val_precision: 0.6364 - val_accuracy: 0.4029\nEpoch 10/60\n269/269 [==============================] - 22s 82ms/step - loss: 1.2769 - recall: 0.0590 - precision: 0.6521 - accuracy: 0.4198 - val_loss: 1.3476 - val_recall: 0.0048 - val_precision: 0.6667 - val_accuracy: 0.3956\nEpoch 11/60\n269/269 [==============================] - 22s 82ms/step - loss: 1.2730 - recall: 0.0601 - precision: 0.6807 - accuracy: 0.4098 - val_loss: 1.3451 - val_recall: 0.0056 - val_precision: 0.6364 - val_accuracy: 0.3956\nEpoch 12/60\n","name":"stdout"},{"output_type":"stream","text":"269/269 [==============================] - 22s 82ms/step - loss: 1.2661 - recall: 0.0578 - precision: 0.6375 - accuracy: 0.4198 - val_loss: 1.3438 - val_recall: 0.0056 - val_precision: 0.6364 - val_accuracy: 0.3948\nEpoch 13/60\n269/269 [==============================] - 22s 82ms/step - loss: 1.2617 - recall: 0.0671 - precision: 0.7024 - accuracy: 0.4219 - val_loss: 1.3420 - val_recall: 0.0048 - val_precision: 0.6000 - val_accuracy: 0.3965\nEpoch 14/60\n269/269 [==============================] - 22s 82ms/step - loss: 1.2580 - recall: 0.0641 - precision: 0.6962 - accuracy: 0.4212 - val_loss: 1.3405 - val_recall: 0.0056 - val_precision: 0.6364 - val_accuracy: 0.3932\nEpoch 15/60\n269/269 [==============================] - 22s 82ms/step - loss: 1.2518 - recall: 0.0704 - precision: 0.7123 - accuracy: 0.4205 - val_loss: 1.3392 - val_recall: 0.0048 - val_precision: 0.6667 - val_accuracy: 0.3932\nEpoch 16/60\n269/269 [==============================] - 22s 82ms/step - loss: 1.2483 - recall: 0.0692 - precision: 0.7333 - accuracy: 0.4263 - val_loss: 1.3331 - val_recall: 0.0097 - val_precision: 0.7500 - val_accuracy: 0.3989\nEpoch 17/60\n269/269 [==============================] - 22s 82ms/step - loss: 1.2436 - recall: 0.0702 - precision: 0.7201 - accuracy: 0.4303 - val_loss: 1.3347 - val_recall: 0.0064 - val_precision: 0.7273 - val_accuracy: 0.3948\nEpoch 18/60\n269/269 [==============================] - 22s 82ms/step - loss: 1.2437 - recall: 0.0653 - precision: 0.7035 - accuracy: 0.4203 - val_loss: 1.3321 - val_recall: 0.0081 - val_precision: 0.7143 - val_accuracy: 0.3940\nEpoch 19/60\n269/269 [==============================] - 22s 83ms/step - loss: 1.2393 - recall: 0.0660 - precision: 0.7128 - accuracy: 0.4228 - val_loss: 1.3323 - val_recall: 0.0073 - val_precision: 0.7500 - val_accuracy: 0.3956\nEpoch 20/60\n269/269 [==============================] - 22s 82ms/step - loss: 1.2366 - recall: 0.0683 - precision: 0.7252 - accuracy: 0.4242 - val_loss: 1.3308 - val_recall: 0.0064 - val_precision: 0.7273 - val_accuracy: 0.3892\nEpoch 21/60\n269/269 [==============================] - 22s 82ms/step - loss: 1.2299 - recall: 0.0716 - precision: 0.7543 - accuracy: 0.4266 - val_loss: 1.3271 - val_recall: 0.0081 - val_precision: 0.7692 - val_accuracy: 0.3956\nEpoch 22/60\n269/269 [==============================] - 22s 82ms/step - loss: 1.2271 - recall: 0.0692 - precision: 0.7351 - accuracy: 0.4259 - val_loss: 1.3231 - val_recall: 0.0105 - val_precision: 0.7647 - val_accuracy: 0.3956\nEpoch 23/60\n269/269 [==============================] - 22s 81ms/step - loss: 1.2193 - recall: 0.0734 - precision: 0.7627 - accuracy: 0.4350 - val_loss: 1.3200 - val_recall: 0.0137 - val_precision: 0.8095 - val_accuracy: 0.3940\nEpoch 24/60\n269/269 [==============================] - 22s 82ms/step - loss: 1.2200 - recall: 0.0711 - precision: 0.7683 - accuracy: 0.4263 - val_loss: 1.3162 - val_recall: 0.0121 - val_precision: 0.7895 - val_accuracy: 0.3900\nEpoch 25/60\n269/269 [==============================] - 22s 82ms/step - loss: 1.2093 - recall: 0.0772 - precision: 0.7422 - accuracy: 0.4322 - val_loss: 1.3194 - val_recall: 0.0089 - val_precision: 0.7857 - val_accuracy: 0.3940\nEpoch 26/60\n269/269 [==============================] - 22s 82ms/step - loss: 1.2124 - recall: 0.0739 - precision: 0.7271 - accuracy: 0.4350 - val_loss: 1.3130 - val_recall: 0.0097 - val_precision: 0.7059 - val_accuracy: 0.3892\nEpoch 27/60\n269/269 [==============================] - 22s 82ms/step - loss: 1.2077 - recall: 0.0737 - precision: 0.7349 - accuracy: 0.4368 - val_loss: 1.3100 - val_recall: 0.0129 - val_precision: 0.8000 - val_accuracy: 0.3876\nEpoch 28/60\n269/269 [==============================] - 22s 82ms/step - loss: 1.2046 - recall: 0.0769 - precision: 0.7728 - accuracy: 0.4333 - val_loss: 1.3160 - val_recall: 0.0113 - val_precision: 0.8235 - val_accuracy: 0.3924\nEpoch 29/60\n269/269 [==============================] - 22s 82ms/step - loss: 1.1988 - recall: 0.0823 - precision: 0.7691 - accuracy: 0.4359 - val_loss: 1.3096 - val_recall: 0.0097 - val_precision: 0.7500 - val_accuracy: 0.3884\nEpoch 30/60\n269/269 [==============================] - 22s 82ms/step - loss: 1.1991 - recall: 0.0790 - precision: 0.7533 - accuracy: 0.4347 - val_loss: 1.3112 - val_recall: 0.0097 - val_precision: 0.8000 - val_accuracy: 0.3892\nEpoch 31/60\n269/269 [==============================] - 22s 82ms/step - loss: 1.1900 - recall: 0.0874 - precision: 0.8030 - accuracy: 0.4415 - val_loss: 1.3060 - val_recall: 0.0145 - val_precision: 0.8182 - val_accuracy: 0.3908\nEpoch 32/60\n269/269 [==============================] - 22s 82ms/step - loss: 1.1895 - recall: 0.0781 - precision: 0.7444 - accuracy: 0.4410 - val_loss: 1.3034 - val_recall: 0.0137 - val_precision: 0.7727 - val_accuracy: 0.3908\nEpoch 33/60\n269/269 [==============================] - 22s 83ms/step - loss: 1.1889 - recall: 0.0837 - precision: 0.7978 - accuracy: 0.4406 - val_loss: 1.2961 - val_recall: 0.0169 - val_precision: 0.7778 - val_accuracy: 0.3892\nEpoch 34/60\n269/269 [==============================] - 22s 81ms/step - loss: 1.1819 - recall: 0.0874 - precision: 0.7748 - accuracy: 0.4431 - val_loss: 1.3002 - val_recall: 0.0137 - val_precision: 0.7727 - val_accuracy: 0.3892\nEpoch 35/60\n125/269 [============>.................] - ETA: 10s - loss: 1.1751 - recall: 0.0835 - precision: 0.7626 - accuracy: 0.4530","name":"stdout"}]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# loading json and model architecture \njson_file = open('./model_json.json', 'r')\nloaded_model_json = json_file.read()\njson_file.close()\nloaded_model = model_from_json(loaded_model_json)\n\n# load weights into new model\nloaded_model.load_weights(\"./saved_models/Emotion_recognition_iemocap.h5\")\nprint(\"Loaded model from disk\")\n \n# Keras optimiser\nopt = keras.optimizers.SGD(lr=0.0001, momentum=0.0, decay=0.0, nesterov=False)\nloaded_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[tf.keras.metrics.Recall(),tf.keras.metrics.Precision(),'accuracy'])\nscore = loaded_model.evaluate(X_test, y_test, verbose=0)\nprint(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n\npreds = loaded_model.predict(X_test, batch_size=16, verbose=1)\n\npreds=preds.argmax(axis=1)\n\nprint(preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = preds.astype(int).flatten()\npreds = (lb.inverse_transform((preds)))\npreds = pd.DataFrame({'predictedvalues': preds})\n\n# Actual labels\nactual=y_test.argmax(axis=1)\nactual = actual.astype(int).flatten()\nactual = (lb.inverse_transform((actual)))\nactual = pd.DataFrame({'actualvalues': actual})\n\n# Lets combined both of them into a single dataframe\nfinaldf = actual.join(preds)\n\n# Write out the predictions to disk\nfinaldf.to_csv('Predictions.csv', index=False)\nprint(finaldf.groupby('predictedvalues').count())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=14):\n    \"\"\"Prints a confusion matrix, as returned by sklearn.metrics.confusion_matrix, as a heatmap.\n    \n    Arguments\n    ---------\n    confusion_matrix: numpy.ndarray\n        The numpy.ndarray object returned from a call to sklearn.metrics.confusion_matrix. \n        Similarly constructed ndarrays can also be used.\n    class_names: list\n        An ordered list of class names, in the order they index the given confusion matrix.\n    figsize: tuple\n        A 2-long tuple, the first value determining the horizontal size of the ouputted figure,\n        the second determining the vertical size. Defaults to (10,7).\n    fontsize: int\n        Font size for axes labels. Defaults to 14.\n        \n    Returns\n    -------\n    matplotlib.figure.Figure\n        The resulting confusion matrix figure\n    \"\"\"\n    df_cm = pd.DataFrame(\n        confusion_matrix, index=class_names, columns=class_names, \n    )\n    fig = plt.figure(figsize=figsize)\n    try:\n        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\n    except ValueError:\n        raise ValueError(\"Confusion matrix values must be integers.\")\n        \n    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"finaldf = pd.read_csv(\"./Predictions.csv\")\nclasses = finaldf.actualvalues.unique()\nclasses.sort()    \nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Confusion matrix \nc = confusion_matrix(finaldf.actualvalues, finaldf.predictedvalues)\nprint(accuracy_score(finaldf.actualvalues, finaldf.predictedvalues))\nplt.show(print_confusion_matrix(c, class_names = classes))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Classification report \nclasses = finaldf.actualvalues.unique()\nclasses.sort()    \nprint(classification_report(finaldf.actualvalues, finaldf.predictedvalues, target_names=classes))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}